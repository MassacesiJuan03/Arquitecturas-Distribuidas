# Gu√≠a de Configuraci√≥n de MPI en Cluster

## üñ•Ô∏è Configuraci√≥n de Cluster con MPI

Esta gu√≠a te ayudar√° a ejecutar los ejercicios MPI en m√∫ltiples computadoras conectadas por red.

## üìã Requisitos

### En TODAS las m√°quinas:
1. **Linux** (Ubuntu, Debian, CentOS, etc.)
2. **Mismo usuario** con el mismo nombre
3. **MPI instalado** (Open MPI recomendado)
4. **Conectividad de red** (ping entre m√°quinas)
5. **SSH habilitado**

## üîß Paso 1: Instalar MPI en Todas las M√°quinas

```bash
# En cada m√°quina del cluster
sudo apt-get update
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev

# Verificar instalaci√≥n
mpirun --version
```

## üîë Paso 2: Configurar SSH sin Contrase√±a

### En la m√°quina principal (desde donde ejecutar√°s mpirun):

```bash
# Generar par de claves SSH (si no existe)
ssh-keygen -t rsa -b 4096
# Presiona Enter para todas las opciones (sin contrase√±a)

# Copiar clave p√∫blica a cada m√°quina remota
ssh-copy-id usuario@192.168.1.101
ssh-copy-id usuario@192.168.1.102
ssh-copy-id usuario@192.168.1.103

# Probar conexi√≥n sin contrase√±a
ssh usuario@192.168.1.101 "hostname"
```

Si `ssh-copy-id` no funciona:
```bash
cat ~/.ssh/id_rsa.pub | ssh usuario@192.168.1.101 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"
```

## üìÅ Paso 3: Crear Archivo de Hosts

Crea `hostfile` en el directorio del proyecto:

```bash
cd tp3
nano hostfile
```

**Opci√≥n A - Usando hostnames:**
```
localhost slots=4
pc1 slots=4
pc2 slots=4
pc3 slots=4
```

**Opci√≥n B - Usando IPs:**
```
192.168.1.100 slots=4
192.168.1.101 slots=4
192.168.1.102 slots=4
192.168.1.103 slots=4
```

**Opci√≥n C - Distribuir espec√≠ficamente:**
```
# M√°quina principal: 2 procesos
localhost slots=2 max_slots=2

# M√°quinas remotas: 2 procesos cada una
192.168.1.101 slots=2 max_slots=2
192.168.1.102 slots=2 max_slots=2
192.168.1.103 slots=2 max_slots=2
```

**Nota:** `slots` = n√∫mero de n√∫cleos/procesos a usar en esa m√°quina

## üöÄ Paso 4: Desplegar el C√≥digo

### Opci√≥n A - Usando el Script Autom√°tico:

```bash
# Editar el script con tus hosts
nano deploy_cluster.sh

# Modificar estas l√≠neas:
HOSTS=("usuario@192.168.1.101" "usuario@192.168.1.102" "usuario@192.168.1.103")

# Dar permisos de ejecuci√≥n
chmod +x deploy_cluster.sh

# Ejecutar despliegue completo
./deploy_cluster.sh --full
```

### Opci√≥n B - Manualmente:

```bash
# Crear directorio en m√°quinas remotas
for host in usuario@192.168.1.101 usuario@192.168.1.102 usuario@192.168.1.103; do
    ssh $host "mkdir -p ~/tp3/code"
done

# Copiar c√≥digo a todas las m√°quinas
for host in usuario@192.168.1.101 usuario@192.168.1.102 usuario@192.168.1.103; do
    rsync -avz code/ $host:~/tp3/code/
done

# Compilar en todas las m√°quinas
for host in usuario@192.168.1.101 usuario@192.168.1.102 usuario@192.168.1.103; do
    ssh $host "cd ~/tp3/code && mpic++ -o ej1_mpi ej1.cpp -std=c++11"
    ssh $host "cd ~/tp3/code && mpic++ -o ej2_mpi ej2.cpp -std=c++11"
    ssh $host "cd ~/tp3/code && mpic++ -o ej3_mpi ej3.cpp -std=c++11"
    ssh $host "cd ~/tp3/code && mpic++ -o ej4_mpi ej4.cpp -std=c++11"
done
```

## ‚ñ∂Ô∏è Paso 5: Ejecutar en el Cluster

```bash
# Ejecutar con archivo hostfile
mpirun --hostfile hostfile -np 8 ~/tp3/code/ej1_mpi

# Ejecutar especificando hosts directamente
mpirun -np 8 --host 192.168.1.101,192.168.1.102,192.168.1.103,localhost ~/tp3/code/ej1_mpi

# Ejercicio 3 (con input)
echo 1000 | mpirun --hostfile hostfile -np 8 ~/tp3/code/ej3_mpi

# Ejercicio 4 (con input)
echo 100000 | mpirun --hostfile hostfile -np 8 ~/tp3/code/ej4_mpi
```

## üìä Ejemplos de Ejecuci√≥n

### Ejemplo 1: 8 procesos distribuidos en 4 m√°quinas (2 por m√°quina)
```bash
mpirun --hostfile hostfile -np 8 ~/tp3/code/ej1_mpi
```

### Ejemplo 2: 16 procesos distribuidos (4 por m√°quina)
```bash
mpirun --hostfile hostfile -np 16 ~/tp3/code/ej4_mpi
```

### Ejemplo 3: Especificar cu√°ntos procesos por host
```bash
mpirun -np 12 \
    --host localhost:4,pc1:4,pc2:4 \
    ~/tp3/code/ej3_mpi
```

## üîç Verificaci√≥n y Debugging

### 1. Verificar conectividad
```bash
# Probar SSH a cada m√°quina
for host in pc1 pc2 pc3; do
    echo -n "Testing $host: "
    ssh $host "hostname" && echo "‚úì" || echo "‚úó"
done
```

### 2. Probar MPI b√°sico
```bash
# Crear programa de prueba
cat > test_mpi.cpp << 'EOF'
#include <mpi.h>
#include <iostream>
#include <unistd.h>
using namespace std;

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    int rank, size;
    char hostname[256];
    
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    gethostname(hostname, sizeof(hostname));
    
    cout << "Proceso " << rank << " de " << size 
         << " ejecut√°ndose en " << hostname << endl;
    
    MPI_Finalize();
    return 0;
}
EOF

# Compilar
mpic++ -o test_mpi test_mpi.cpp

# Copiar a otras m√°quinas
for host in usuario@pc1 usuario@pc2 usuario@pc3; do
    scp test_mpi $host:~/
done

# Ejecutar
mpirun --hostfile hostfile -np 8 ~/test_mpi
```

**Salida esperada:**
```
Proceso 0 de 8 ejecut√°ndose en localhost
Proceso 1 de 8 ejecut√°ndose en localhost
Proceso 2 de 8 ejecut√°ndose en pc1
Proceso 3 de 8 ejecut√°ndose en pc1
Proceso 4 de 8 ejecut√°ndose en pc2
Proceso 5 de 8 ejecut√°ndose en pc2
Proceso 6 de 8 ejecut√°ndose en pc3
Proceso 7 de 8 ejecut√°ndose en pc3
```

### 3. Variables de entorno √∫tiles
```bash
# Mostrar m√°s informaci√≥n de debug
export OMPI_MCA_plm_base_verbose=100

# Si hay problemas con shared memory en WSL/VM
export OMPI_MCA_btl_vader_single_copy_mechanism=none

# Agregar a ~/.bashrc para hacerlo permanente
echo 'export OMPI_MCA_btl_vader_single_copy_mechanism=none' >> ~/.bashrc
```

## ‚ùå Soluci√≥n de Problemas

### Error: "Permission denied"
```bash
# Verificar permisos de ~/.ssh
chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys
```

### Error: "Host key verification failed"
```bash
# Agregar hosts a known_hosts
for host in pc1 pc2 pc3; do
    ssh-keyscan -H $host >> ~/.ssh/known_hosts
done
```

### Error: "mpirun cannot find executable"
```bash
# Usar rutas absolutas
mpirun --hostfile hostfile -np 8 /home/usuario/tp3/code/ej1_mpi
```

### Error: Versiones diferentes de MPI
```bash
# Verificar versi√≥n en cada m√°quina
for host in pc1 pc2 pc3; do
    echo "$host:"
    ssh $host "mpirun --version"
done

# Instalar la misma versi√≥n en todas
```

## üìà Optimizaci√≥n de Rendimiento

### 1. Usar binding de CPU
```bash
# Asignar procesos a n√∫cleos espec√≠ficos
mpirun --bind-to core --hostfile hostfile -np 8 ~/tp3/code/ej1_mpi
```

### 2. Configurar timeout
```bash
# Timeout de 5 minutos
mpirun --timeout 300 --hostfile hostfile -np 8 ~/tp3/code/ej4_mpi
```

### 3. Mapeo personalizado
```bash
# 4 procesos en pc1, 2 en pc2, 2 en pc3
mpirun --host pc1:4,pc2:2,pc3:2 -np 8 ~/tp3/code/ej1_mpi
```

## üåê Configuraci√≥n Avanzada: NFS (Opcional)

Si quieres que todas las m√°quinas vean la misma carpeta:

```bash
# En el servidor (m√°quina principal)
sudo apt-get install nfs-kernel-server
sudo mkdir -p /export/tp3
sudo cp -r ~/tp3/* /export/tp3/

# Editar /etc/exports
echo "/export/tp3 192.168.1.0/24(rw,sync,no_subtree_check)" | sudo tee -a /etc/exports

sudo exportfs -a
sudo systemctl restart nfs-kernel-server

# En cada cliente
sudo apt-get install nfs-common
sudo mkdir -p /mnt/tp3
sudo mount 192.168.1.100:/export/tp3 /mnt/tp3

# Agregar a /etc/fstab para montaje autom√°tico
echo "192.168.1.100:/export/tp3 /mnt/tp3 nfs defaults 0 0" | sudo tee -a /etc/fstab
```

## üìù Checklist Final

- [ ] MPI instalado en todas las m√°quinas
- [ ] SSH sin contrase√±a configurado
- [ ] Archivo hostfile creado
- [ ] C√≥digo copiado a todas las m√°quinas
- [ ] C√≥digo compilado en todas las m√°quinas
- [ ] Test b√°sico de MPI funciona
- [ ] Rutas absolutas en comandos mpirun

¬°Listo para ejecutar en cluster! üöÄ
